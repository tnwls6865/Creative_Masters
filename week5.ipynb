{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMNELIkSWwfS0D1yUo5lLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnwls6865/Creative_Masters/blob/main/week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creative Masters"
      ],
      "metadata": {
        "id": "uI8D8X0Q9_Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Chat GPT\n",
        "https://openai.com/blog/chatgpt"
      ],
      "metadata": {
        "id": "rGXMhOp3-b0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 간단한 계산기"
      ],
      "metadata": {
        "id": "g0A9J65c-oeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 정의\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "\n",
        "def subtract(x, y):\n",
        "    return x - y\n",
        "\n",
        "def multiply(x, y):\n",
        "    return x * y\n",
        "\n",
        "def divide(x, y):\n",
        "    return x / y\n",
        "\n",
        "# 사용자 입력 받기\n",
        "print(\"사칙연산 계산기입니다.\")\n",
        "print(\"사용 가능한 연산: +, -, *, /\")\n",
        "num1 = float(input(\"첫 번째 숫자를 입력하세요: \"))\n",
        "num2 = float(input(\"두 번째 숫자를 입력하세요: \"))\n",
        "operator = input(\"연산자를 입력하세요: \")\n",
        "\n",
        "# 계산 수행\n",
        "if operator == \"+\":\n",
        "    print(num1, \"+\", num2, \"=\", add(num1, num2))\n",
        "\n",
        "elif operator == \"-\":\n",
        "    print(num1, \"-\", num2, \"=\", subtract(num1, num2))\n",
        "\n",
        "elif operator == \"*\":\n",
        "    print(num1, \"*\", num2, \"=\", multiply(num1, num2))\n",
        "\n",
        "elif operator == \"/\":\n",
        "    print(num1, \"/\", num2, \"=\", divide(num1, num2))\n",
        "\n",
        "else:\n",
        "    print(\"올바른 연산자를 입력하세요.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXKrBj9P-ANC",
        "outputId": "cd0aa181-3af1-4952-fe6b-20f6917b205e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사칙연산 계산기입니다.\n",
            "사용 가능한 연산: +, -, *, /\n",
            "첫 번째 숫자를 입력하세요: 5\n",
            "두 번째 숫자를 입력하세요: 10\n",
            "연산자를 입력하세요: /\n",
            "5.0 / 10.0 = 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### python 에서 numpy array 연산 예시 보여줘 "
      ],
      "metadata": {
        "id": "48sQthOHCb-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1차원 배열 생성하기\n",
        "a = np.array([1, 2, 3])\n",
        "\n",
        "# 2차원 배열 생성하기\n",
        "b = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# 3차원 배열 생성하기\n",
        "c = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
      ],
      "metadata": {
        "id": "nTvgfgiw-mwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "# 배열 더하기\n",
        "c = a + b  # [5, 7, 9]\n",
        "\n",
        "# 배열 빼기\n",
        "d = a - b  # [-3, -3, -3]\n",
        "\n",
        "# 배열 곱하기 (요소별 곱셈)\n",
        "e = a * b  # [4, 10, 18]\n",
        "\n",
        "# 배열 나누기 (요소별 나눗셈)\n",
        "f = b / a  # [4.0, 2.5, 2.0]\n",
        "\n",
        "# 배열 제곱\n",
        "g = a ** 2  # [1, 4, 9]\n",
        "\n",
        "# 배열 비교 (요소별 비교)\n",
        "h = a > b  # [False, False, False]"
      ],
      "metadata": {
        "id": "23hMosGnCmY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(e)\n",
        "print(f)\n",
        "print(g)\n",
        "print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js7b3--bCnpk",
        "outputId": "9d63cc5e-1c86-4533-a816-492bc333f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "[4 5 6]\n",
            "[5 7 9]\n",
            "[-3 -3 -3]\n",
            "[ 4 10 18]\n",
            "[4.  2.5 2. ]\n",
            "[1 4 9]\n",
            "[False False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### python으로 knn코드 짜줘"
      ],
      "metadata": {
        "id": "FXDA80n7Duwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 거리 계산 함수\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # 거리 계산\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # k개의 최근접 이웃 구하기\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # 가장 투표를 많이 받은 레이블 반환\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n"
      ],
      "metadata": {
        "id": "P3TRUDY3CvEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### pytorch cifar10 dataset으로 knn학습하는 코드 짜줘"
      ],
      "metadata": {
        "id": "2V_Y3b5dD4uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# CIFAR10 데이터셋 다운로드 및 전처리\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 나누기\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50000, shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False)\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "def preprocess_data(dataloader):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for batch in dataloader:\n",
        "        input_batch, label_batch = batch\n",
        "        input_batch = input_batch.view(input_batch.shape[0], -1)\n",
        "        inputs.append(input_batch.numpy())\n",
        "        labels.append(label_batch.numpy())\n",
        "    inputs = np.concatenate(inputs, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return inputs, labels\n",
        "\n",
        "# 데이터 전처리\n",
        "X_train, y_train = preprocess_data(trainloader)\n",
        "X_test, y_test = preprocess_data(testloader)\n",
        "\n",
        "# 거리 계산 함수\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # 거리 계산\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # k개의 최근접 이웃 구하기\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # 가장 투표를 많이 받은 레이블 반환\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        return most_common[0][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dDo5CPEDQVj",
        "outputId": "6bf5049b-d95d-4cf1-9173-38b8a44e289a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN 모델 생성 및 학습\n",
        "knn = KNN(k=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터로 예측하기\n",
        "y_pred = knn.predict(X_test[:50])\n",
        "\n",
        "# 정확도 계산하기\n",
        "accuracy = np.mean(y_pred == y_test[:50])\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "H8BMvMuvD9dA",
        "outputId": "329aecaf-eee1-404c-f5ff-455a3ec720d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch로 linear classifier코드 짜줘"
      ],
      "metadata": {
        "id": "ds0aV6J6SaUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1. 데이터 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = datasets.MNIST('./data/', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = datasets.MNIST('./data/', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. 모델 아키텍처 정의\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(784, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = LinearClassifier()\n",
        "\n",
        "# 3. 손실 함수 선택\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. 최적화 알고리즘 선택\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "# 5. 모델 학습\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in testloader:\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(testloader.dataset),\n",
        "        100. * correct / len(testloader.dataset)))\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, trainloader, criterion, optimizer)\n",
        "    test(model, testloader, criterion)\n",
        "\n",
        "# 6. 모델 평가\n",
        "test(model, testloader, criterion)\n"
      ],
      "metadata": {
        "id": "wP6C6jNpSdqW",
        "outputId": "b48cc991-5dcd-4e00-8d9e-3e23d394928f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 94698158.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 17808834.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28550692.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9942864.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0048, Accuracy: 9124/10000 (91%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0046, Accuracy: 9177/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 9207/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 9207/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 9195/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 9237/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 9204/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0044, Accuracy: 9199/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 9239/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 9222/10000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0043, Accuracy: 9222/10000 (92%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# 데이터셋 불러오기 및 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 모델 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 320)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화\n",
        "model = CNN()\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 모델 학습\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 순전파\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # 역전파 및 가중치 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 로그 출력\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, batch_idx+1, len(train_loader), loss.item()))\n",
        "\n",
        "# 모델 평가\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, targets in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "id": "6WHeah2gShIB",
        "outputId": "f7568536-5e6c-4b9a-a0fd-56eb6f2e4125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [1/938], Loss: 2.3327\n",
            "Epoch [1/10], Batch [101/938], Loss: 0.2711\n",
            "Epoch [1/10], Batch [201/938], Loss: 0.1644\n",
            "Epoch [1/10], Batch [301/938], Loss: 0.2820\n",
            "Epoch [1/10], Batch [401/938], Loss: 0.5163\n",
            "Epoch [1/10], Batch [501/938], Loss: 0.1074\n",
            "Epoch [1/10], Batch [601/938], Loss: 0.1975\n",
            "Epoch [1/10], Batch [701/938], Loss: 0.1146\n",
            "Epoch [1/10], Batch [801/938], Loss: 0.0947\n",
            "Epoch [1/10], Batch [901/938], Loss: 0.0259\n",
            "Epoch [2/10], Batch [1/938], Loss: 0.0448\n",
            "Epoch [2/10], Batch [101/938], Loss: 0.0509\n",
            "Epoch [2/10], Batch [201/938], Loss: 0.1146\n",
            "Epoch [2/10], Batch [301/938], Loss: 0.0801\n",
            "Epoch [2/10], Batch [401/938], Loss: 0.0522\n",
            "Epoch [2/10], Batch [501/938], Loss: 0.2500\n",
            "Epoch [2/10], Batch [601/938], Loss: 0.0252\n",
            "Epoch [2/10], Batch [701/938], Loss: 0.0463\n",
            "Epoch [2/10], Batch [801/938], Loss: 0.0908\n",
            "Epoch [2/10], Batch [901/938], Loss: 0.1231\n",
            "Epoch [3/10], Batch [1/938], Loss: 0.0714\n",
            "Epoch [3/10], Batch [101/938], Loss: 0.0433\n",
            "Epoch [3/10], Batch [201/938], Loss: 0.0640\n",
            "Epoch [3/10], Batch [301/938], Loss: 0.0639\n",
            "Epoch [3/10], Batch [401/938], Loss: 0.0042\n",
            "Epoch [3/10], Batch [501/938], Loss: 0.0183\n",
            "Epoch [3/10], Batch [601/938], Loss: 0.1310\n",
            "Epoch [3/10], Batch [701/938], Loss: 0.0242\n",
            "Epoch [3/10], Batch [801/938], Loss: 0.0372\n",
            "Epoch [3/10], Batch [901/938], Loss: 0.0061\n",
            "Epoch [4/10], Batch [1/938], Loss: 0.0059\n",
            "Epoch [4/10], Batch [101/938], Loss: 0.0356\n",
            "Epoch [4/10], Batch [201/938], Loss: 0.1283\n",
            "Epoch [4/10], Batch [301/938], Loss: 0.0217\n",
            "Epoch [4/10], Batch [401/938], Loss: 0.0551\n",
            "Epoch [4/10], Batch [501/938], Loss: 0.0057\n",
            "Epoch [4/10], Batch [601/938], Loss: 0.1067\n",
            "Epoch [4/10], Batch [701/938], Loss: 0.0167\n",
            "Epoch [4/10], Batch [801/938], Loss: 0.0162\n",
            "Epoch [4/10], Batch [901/938], Loss: 0.0641\n",
            "Epoch [5/10], Batch [1/938], Loss: 0.0267\n",
            "Epoch [5/10], Batch [101/938], Loss: 0.0064\n",
            "Epoch [5/10], Batch [201/938], Loss: 0.1405\n",
            "Epoch [5/10], Batch [301/938], Loss: 0.0844\n",
            "Epoch [5/10], Batch [401/938], Loss: 0.0233\n",
            "Epoch [5/10], Batch [501/938], Loss: 0.0221\n",
            "Epoch [5/10], Batch [601/938], Loss: 0.0270\n",
            "Epoch [5/10], Batch [701/938], Loss: 0.0017\n",
            "Epoch [5/10], Batch [801/938], Loss: 0.1152\n",
            "Epoch [5/10], Batch [901/938], Loss: 0.0593\n",
            "Epoch [6/10], Batch [1/938], Loss: 0.0133\n",
            "Epoch [6/10], Batch [101/938], Loss: 0.0535\n",
            "Epoch [6/10], Batch [201/938], Loss: 0.0142\n",
            "Epoch [6/10], Batch [301/938], Loss: 0.0060\n",
            "Epoch [6/10], Batch [401/938], Loss: 0.0030\n",
            "Epoch [6/10], Batch [501/938], Loss: 0.0047\n",
            "Epoch [6/10], Batch [601/938], Loss: 0.0805\n",
            "Epoch [6/10], Batch [701/938], Loss: 0.0021\n",
            "Epoch [6/10], Batch [801/938], Loss: 0.0130\n",
            "Epoch [6/10], Batch [901/938], Loss: 0.0171\n",
            "Epoch [7/10], Batch [1/938], Loss: 0.0025\n",
            "Epoch [7/10], Batch [101/938], Loss: 0.0607\n",
            "Epoch [7/10], Batch [201/938], Loss: 0.0568\n",
            "Epoch [7/10], Batch [301/938], Loss: 0.0284\n",
            "Epoch [7/10], Batch [401/938], Loss: 0.0065\n",
            "Epoch [7/10], Batch [501/938], Loss: 0.0036\n",
            "Epoch [7/10], Batch [601/938], Loss: 0.1037\n",
            "Epoch [7/10], Batch [701/938], Loss: 0.0151\n",
            "Epoch [7/10], Batch [801/938], Loss: 0.0255\n",
            "Epoch [7/10], Batch [901/938], Loss: 0.0157\n",
            "Epoch [8/10], Batch [1/938], Loss: 0.0085\n",
            "Epoch [8/10], Batch [101/938], Loss: 0.0132\n",
            "Epoch [8/10], Batch [201/938], Loss: 0.0561\n",
            "Epoch [8/10], Batch [301/938], Loss: 0.0159\n",
            "Epoch [8/10], Batch [401/938], Loss: 0.0031\n",
            "Epoch [8/10], Batch [501/938], Loss: 0.0041\n",
            "Epoch [8/10], Batch [601/938], Loss: 0.0209\n",
            "Epoch [8/10], Batch [701/938], Loss: 0.0834\n",
            "Epoch [8/10], Batch [801/938], Loss: 0.0030\n",
            "Epoch [8/10], Batch [901/938], Loss: 0.0128\n",
            "Epoch [9/10], Batch [1/938], Loss: 0.0097\n",
            "Epoch [9/10], Batch [101/938], Loss: 0.0041\n",
            "Epoch [9/10], Batch [201/938], Loss: 0.0056\n",
            "Epoch [9/10], Batch [301/938], Loss: 0.0431\n",
            "Epoch [9/10], Batch [401/938], Loss: 0.0215\n",
            "Epoch [9/10], Batch [501/938], Loss: 0.0816\n",
            "Epoch [9/10], Batch [601/938], Loss: 0.0006\n",
            "Epoch [9/10], Batch [701/938], Loss: 0.0001\n",
            "Epoch [9/10], Batch [801/938], Loss: 0.0220\n",
            "Epoch [9/10], Batch [901/938], Loss: 0.0163\n",
            "Epoch [10/10], Batch [1/938], Loss: 0.0144\n",
            "Epoch [10/10], Batch [101/938], Loss: 0.0467\n",
            "Epoch [10/10], Batch [201/938], Loss: 0.0536\n",
            "Epoch [10/10], Batch [301/938], Loss: 0.0124\n",
            "Epoch [10/10], Batch [401/938], Loss: 0.0023\n",
            "Epoch [10/10], Batch [501/938], Loss: 0.0168\n",
            "Epoch [10/10], Batch [601/938], Loss: 0.2005\n",
            "Epoch [10/10], Batch [701/938], Loss: 0.0006\n",
            "Epoch [10/10], Batch [801/938], Loss: 0.0030\n",
            "Epoch [10/10], Batch [901/938], Loss: 0.0473\n",
            "Test Accuracy: 98.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# 데이터셋 불러오기 및 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 모델 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 320)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화\n",
        "model = CNN()\n",
        "model = model.cuda()\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 정의\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 모델 학습\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 순전파\n",
        "        outputs = model(data.cuda())\n",
        "        loss = criterion(outputs, targets.cuda())\n",
        "        \n",
        "        # 역전파 및 가중치 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 로그 출력\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, batch_idx+1, len(train_loader), loss.item()))\n",
        "\n",
        "# 모델 평가\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, targets in test_loader:\n",
        "        outputs = model(data.cuda())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "id": "pNvSIDGbZN8Q",
        "outputId": "0c8bbe8b-95d6-4c0b-a045-cdd243c8ad01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [1/938], Loss: 2.3195\n",
            "Epoch [1/10], Batch [101/938], Loss: 0.2572\n",
            "Epoch [1/10], Batch [201/938], Loss: 0.1884\n",
            "Epoch [1/10], Batch [301/938], Loss: 0.1980\n",
            "Epoch [1/10], Batch [401/938], Loss: 0.1768\n",
            "Epoch [1/10], Batch [501/938], Loss: 0.0616\n",
            "Epoch [1/10], Batch [601/938], Loss: 0.1251\n",
            "Epoch [1/10], Batch [701/938], Loss: 0.0102\n",
            "Epoch [1/10], Batch [801/938], Loss: 0.0178\n",
            "Epoch [1/10], Batch [901/938], Loss: 0.0679\n",
            "Epoch [2/10], Batch [1/938], Loss: 0.0538\n",
            "Epoch [2/10], Batch [101/938], Loss: 0.0364\n",
            "Epoch [2/10], Batch [201/938], Loss: 0.1704\n",
            "Epoch [2/10], Batch [301/938], Loss: 0.0571\n",
            "Epoch [2/10], Batch [401/938], Loss: 0.0366\n",
            "Epoch [2/10], Batch [501/938], Loss: 0.0602\n",
            "Epoch [2/10], Batch [601/938], Loss: 0.0083\n",
            "Epoch [2/10], Batch [701/938], Loss: 0.0501\n",
            "Epoch [2/10], Batch [801/938], Loss: 0.0280\n",
            "Epoch [2/10], Batch [901/938], Loss: 0.0192\n",
            "Epoch [3/10], Batch [1/938], Loss: 0.0111\n",
            "Epoch [3/10], Batch [101/938], Loss: 0.0117\n",
            "Epoch [3/10], Batch [201/938], Loss: 0.1521\n",
            "Epoch [3/10], Batch [301/938], Loss: 0.0698\n",
            "Epoch [3/10], Batch [401/938], Loss: 0.0186\n",
            "Epoch [3/10], Batch [501/938], Loss: 0.0239\n",
            "Epoch [3/10], Batch [601/938], Loss: 0.0086\n",
            "Epoch [3/10], Batch [701/938], Loss: 0.0113\n",
            "Epoch [3/10], Batch [801/938], Loss: 0.0017\n",
            "Epoch [3/10], Batch [901/938], Loss: 0.0071\n",
            "Epoch [4/10], Batch [1/938], Loss: 0.0480\n",
            "Epoch [4/10], Batch [101/938], Loss: 0.0473\n",
            "Epoch [4/10], Batch [201/938], Loss: 0.0105\n",
            "Epoch [4/10], Batch [301/938], Loss: 0.1150\n",
            "Epoch [4/10], Batch [401/938], Loss: 0.0474\n",
            "Epoch [4/10], Batch [501/938], Loss: 0.0963\n",
            "Epoch [4/10], Batch [601/938], Loss: 0.0962\n",
            "Epoch [4/10], Batch [701/938], Loss: 0.0668\n",
            "Epoch [4/10], Batch [801/938], Loss: 0.0775\n",
            "Epoch [4/10], Batch [901/938], Loss: 0.0054\n",
            "Epoch [5/10], Batch [1/938], Loss: 0.0180\n",
            "Epoch [5/10], Batch [101/938], Loss: 0.0094\n",
            "Epoch [5/10], Batch [201/938], Loss: 0.0715\n",
            "Epoch [5/10], Batch [301/938], Loss: 0.0190\n",
            "Epoch [5/10], Batch [401/938], Loss: 0.0054\n",
            "Epoch [5/10], Batch [501/938], Loss: 0.0912\n",
            "Epoch [5/10], Batch [601/938], Loss: 0.0063\n",
            "Epoch [5/10], Batch [701/938], Loss: 0.0369\n",
            "Epoch [5/10], Batch [801/938], Loss: 0.0070\n",
            "Epoch [5/10], Batch [901/938], Loss: 0.0166\n",
            "Epoch [6/10], Batch [1/938], Loss: 0.0010\n",
            "Epoch [6/10], Batch [101/938], Loss: 0.0004\n",
            "Epoch [6/10], Batch [201/938], Loss: 0.0039\n",
            "Epoch [6/10], Batch [301/938], Loss: 0.0021\n",
            "Epoch [6/10], Batch [401/938], Loss: 0.0237\n",
            "Epoch [6/10], Batch [501/938], Loss: 0.0188\n",
            "Epoch [6/10], Batch [601/938], Loss: 0.0163\n",
            "Epoch [6/10], Batch [701/938], Loss: 0.1787\n",
            "Epoch [6/10], Batch [801/938], Loss: 0.1199\n",
            "Epoch [6/10], Batch [901/938], Loss: 0.0087\n",
            "Epoch [7/10], Batch [1/938], Loss: 0.0320\n",
            "Epoch [7/10], Batch [101/938], Loss: 0.0013\n",
            "Epoch [7/10], Batch [201/938], Loss: 0.0051\n",
            "Epoch [7/10], Batch [301/938], Loss: 0.0016\n",
            "Epoch [7/10], Batch [401/938], Loss: 0.0758\n",
            "Epoch [7/10], Batch [501/938], Loss: 0.0448\n",
            "Epoch [7/10], Batch [601/938], Loss: 0.0115\n",
            "Epoch [7/10], Batch [701/938], Loss: 0.0048\n",
            "Epoch [7/10], Batch [801/938], Loss: 0.0102\n",
            "Epoch [7/10], Batch [901/938], Loss: 0.0592\n",
            "Epoch [8/10], Batch [1/938], Loss: 0.0011\n",
            "Epoch [8/10], Batch [101/938], Loss: 0.0051\n",
            "Epoch [8/10], Batch [201/938], Loss: 0.0003\n",
            "Epoch [8/10], Batch [301/938], Loss: 0.0021\n",
            "Epoch [8/10], Batch [401/938], Loss: 0.0368\n",
            "Epoch [8/10], Batch [501/938], Loss: 0.0009\n",
            "Epoch [8/10], Batch [601/938], Loss: 0.0046\n",
            "Epoch [8/10], Batch [701/938], Loss: 0.0060\n",
            "Epoch [8/10], Batch [801/938], Loss: 0.0161\n",
            "Epoch [8/10], Batch [901/938], Loss: 0.0263\n",
            "Epoch [9/10], Batch [1/938], Loss: 0.0095\n",
            "Epoch [9/10], Batch [101/938], Loss: 0.0015\n",
            "Epoch [9/10], Batch [201/938], Loss: 0.0014\n",
            "Epoch [9/10], Batch [301/938], Loss: 0.0317\n",
            "Epoch [9/10], Batch [401/938], Loss: 0.0075\n",
            "Epoch [9/10], Batch [501/938], Loss: 0.0015\n",
            "Epoch [9/10], Batch [601/938], Loss: 0.0134\n",
            "Epoch [9/10], Batch [701/938], Loss: 0.0003\n",
            "Epoch [9/10], Batch [801/938], Loss: 0.0023\n",
            "Epoch [9/10], Batch [901/938], Loss: 0.0215\n",
            "Epoch [10/10], Batch [1/938], Loss: 0.0175\n",
            "Epoch [10/10], Batch [101/938], Loss: 0.0918\n",
            "Epoch [10/10], Batch [201/938], Loss: 0.0066\n",
            "Epoch [10/10], Batch [301/938], Loss: 0.0115\n",
            "Epoch [10/10], Batch [401/938], Loss: 0.0049\n",
            "Epoch [10/10], Batch [501/938], Loss: 0.0182\n",
            "Epoch [10/10], Batch [601/938], Loss: 0.0054\n",
            "Epoch [10/10], Batch [701/938], Loss: 0.0075\n",
            "Epoch [10/10], Batch [801/938], Loss: 0.0430\n",
            "Epoch [10/10], Batch [901/938], Loss: 0.0081\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-16fd2f9faeae>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy: {:.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aH0pb_X5ctG3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}